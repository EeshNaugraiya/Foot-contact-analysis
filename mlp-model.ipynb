{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef166f6d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-30T16:49:41.763815Z",
     "iopub.status.busy": "2024-09-30T16:49:41.763461Z",
     "iopub.status.idle": "2024-09-30T16:49:55.522554Z",
     "shell.execute_reply": "2024-09-30T16:49:55.521218Z"
    },
    "papermill": {
     "duration": 13.767575,
     "end_time": "2024-09-30T16:49:55.525883",
     "exception": false,
     "start_time": "2024-09-30T16:49:41.758308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 16:49:44.515600: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-30 16:49:44.515705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-30 16:49:44.654451: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential, Model, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score, f1_score, matthews_corrcoef, confusion_matrix\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fad2f34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T16:49:55.539202Z",
     "iopub.status.busy": "2024-09-30T16:49:55.537954Z",
     "iopub.status.idle": "2024-09-30T16:49:55.547538Z",
     "shell.execute_reply": "2024-09-30T16:49:55.546529Z"
    },
    "papermill": {
     "duration": 0.018413,
     "end_time": "2024-09-30T16:49:55.550005",
     "exception": false,
     "start_time": "2024-09-30T16:49:55.531592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to create directory if it does not exist\n",
    "def create_directory(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# Base path to the synthetic dataset\n",
    "base_path = '/kaggle/input/movement-dataset-entire/synthetic_dataset'\n",
    "# Path to save the frames\n",
    "frames_base_dir = '/kaggle/working/frames'\n",
    "test_dir = '/kaggle/input/test-movement-dataset/input_videos'\n",
    "frames_test_dir = '/kaggle/working/test-frames'\n",
    "\n",
    "# Create the directory to save frames\n",
    "create_directory(frames_base_dir)\n",
    "create_directory(frames_test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "559cd827",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T16:49:55.560948Z",
     "iopub.status.busy": "2024-09-30T16:49:55.560211Z",
     "iopub.status.idle": "2024-09-30T16:50:26.696722Z",
     "shell.execute_reply": "2024-09-30T16:50:26.695689Z"
    },
    "papermill": {
     "duration": 31.144484,
     "end_time": "2024-09-30T16:50:26.699005",
     "exception": false,
     "start_time": "2024-09-30T16:49:55.554521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 60 frames from 449_swing_dancing_view0.mp4\n",
      "Processed 60 frames from 1847_baseball_walk_in_view0.mp4\n",
      "Processed 60 frames from 378_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 321_boxing_view0.mp4\n",
      "Processed 60 frames from 1854_baseball_step_up_to_bat_view0.mp4\n",
      "Processed 60 frames from 43_walking_view0.mp4\n",
      "Processed 60 frames from 189_baseball_pitching_view0.mp4\n",
      "Processed 60 frames from 391_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 2166_standing_turn_90_right_view0.mp4\n",
      "Processed 60 frames from 376_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 1855_baseball_milling_idle_view0.mp4\n",
      "Processed 60 frames from 2165_standing_turn_90_left_view0.mp4\n",
      "Processed 60 frames from 448_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 392_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 326_samba_dancing_view0.mp4\n",
      "Processed 60 frames from 1851_baseball_hit_view0.mp4\n",
      "Processed 60 frames from 1529_quarterback_pass_view0.mp4\n",
      "Processed 60 frames from 328_boxing_view0.mp4\n",
      "Processed 60 frames from 453_swing_dancing_view0.mp4\n",
      "Processed 60 frames from 1857_baseball_bunt_view0.mp4\n",
      "Processed 60 frames from 1856_baseball_strike_view0.mp4\n",
      "Processed 60 frames from 340_boxing_view0.mp4\n",
      "Processed 60 frames from 28_walking_view0.mp4\n",
      "Processed 60 frames from 27_walking_view0.mp4\n",
      "Processed 60 frames from 42_crouched_walking_view0.mp4\n",
      "Processed 60 frames from 1846_baseball_step_out_view0.mp4\n",
      "Processed 60 frames from 451_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 2202_standing_turn_left_90_view0.mp4\n",
      "Processed 60 frames from 44_walking_view0.mp4\n",
      "Processed 60 frames from 1844_baseball_idle_view0.mp4\n",
      "Processed 60 frames from 25_walking_view0.mp4\n",
      "Processed 60 frames from 373_samba_dancing_view0.mp4\n",
      "Processed 60 frames from 116_baseball_catcher_view0.mp4\n",
      "Processed 60 frames from 26_walking_view0.mp4\n",
      "Processed 60 frames from 396_samba_dancing_view0.mp4\n",
      "Processed 60 frames from 39_walking_view0.mp4\n",
      "Processed 60 frames from 293_swing_dancing_view0.mp4\n",
      "Processed 60 frames from 286_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 1245_idle_view0.mp4\n",
      "Processed 60 frames from 1848_baseball_walk_out_view0.mp4\n",
      "Processed 60 frames from 1845_baseball_step_in_view0.mp4\n",
      "Processed 60 frames from 1850_baseball_hit_view0.mp4\n",
      "Processed 60 frames from 327_samba_dancing_view0.mp4\n",
      "Processed 60 frames from 1246_idle_view0.mp4\n",
      "Processed 60 frames from 444_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 119_baseball_swing_view0.mp4\n",
      "Processed 60 frames from 131_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 2203_standing_turn_right_90_view0.mp4\n",
      "Processed 60 frames from 118_baseball_idle_view0.mp4\n",
      "Processed 60 frames from 393_samba_dancing_view0.mp4\n",
      "Processed 60 frames from 277_samba_dancing_view0.mp4\n",
      "Processed 60 frames from 1250_idle_view0.mp4\n",
      "Processed 60 frames from 1473_baseball_pitching_view0.mp4\n",
      "Processed 60 frames from 301_samba_dancing_view0.mp4\n",
      "Processed 60 frames from 445_swing_dancing_view0.mp4\n",
      "Processed 60 frames from 1849_baseball_hit_view0.mp4\n",
      "Processed 60 frames from 117_baseball_bunt_view0.mp4\n",
      "Processed 60 frames from 1192_idle_view0.mp4\n",
      "Processed 60 frames from 115_baseball_catcher_view0.mp4\n",
      "Processed 60 frames from 443_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 303_swing_dancing_view0.mp4\n",
      "Processed 60 frames from 285_samba_dancing_view0.mp4\n",
      "Processed 60 frames from 121_baseball_umpire_view0.mp4\n",
      "Processed 60 frames from 120_baseball_swing_inside_view0.mp4\n",
      "Processed 60 frames from 24_walking_view0.mp4\n",
      "Processed 60 frames from 449_swing_dancing_view0.mp4\n",
      "Processed 60 frames from 1847_baseball_walk_in_view0.mp4\n",
      "Processed 60 frames from 378_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 321_boxing_view0.mp4\n",
      "Processed 60 frames from 1854_baseball_step_up_to_bat_view0.mp4\n",
      "Processed 60 frames from 43_walking_view0.mp4\n",
      "Processed 60 frames from 189_baseball_pitching_view0.mp4\n",
      "Processed 60 frames from 391_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 2166_standing_turn_90_right_view0.mp4\n",
      "Processed 60 frames from 376_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 1855_baseball_milling_idle_view0.mp4\n",
      "Processed 60 frames from 2165_standing_turn_90_left_view0.mp4\n",
      "Processed 60 frames from 448_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 392_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 326_samba_dancing_view0.mp4\n",
      "Processed 60 frames from 1851_baseball_hit_view0.mp4\n",
      "Processed 60 frames from 1529_quarterback_pass_view0.mp4\n",
      "Processed 60 frames from 328_boxing_view0.mp4\n",
      "Processed 60 frames from 453_swing_dancing_view0.mp4\n",
      "Processed 60 frames from 1857_baseball_bunt_view0.mp4\n",
      "Processed 60 frames from 1856_baseball_strike_view0.mp4\n",
      "Processed 60 frames from 340_boxing_view0.mp4\n",
      "Processed 60 frames from 28_walking_view0.mp4\n",
      "Processed 60 frames from 27_walking_view0.mp4\n",
      "Processed 60 frames from 42_crouched_walking_view0.mp4\n",
      "Processed 60 frames from 1846_baseball_step_out_view0.mp4\n",
      "Processed 60 frames from 451_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 2202_standing_turn_left_90_view0.mp4\n",
      "Processed 60 frames from 44_walking_view0.mp4\n",
      "Processed 60 frames from 1844_baseball_idle_view0.mp4\n",
      "Processed 60 frames from 25_walking_view0.mp4\n",
      "Processed 60 frames from 373_samba_dancing_view0.mp4\n",
      "Processed 60 frames from 116_baseball_catcher_view0.mp4\n",
      "Processed 60 frames from 26_walking_view0.mp4\n",
      "Reached the frame limit of 6000.\n",
      "Processed 60 frames from 396_samba_dancing_view0.mp4\n",
      "                                            filename  left_heel  left_toe  \\\n",
      "0  /kaggle/working/frames/44944_Stefani_449_swing...          0         0   \n",
      "1  /kaggle/working/frames/44944_Stefani_449_swing...          0         0   \n",
      "2  /kaggle/working/frames/44944_Stefani_449_swing...          0         0   \n",
      "3  /kaggle/working/frames/44944_Stefani_449_swing...          0         0   \n",
      "4  /kaggle/working/frames/44944_Stefani_449_swing...          0         0   \n",
      "\n",
      "   right_heel  right_toe  \n",
      "0           1          1  \n",
      "1           1          1  \n",
      "2           1          1  \n",
      "3           1          1  \n",
      "4           1          1  \n",
      "                                            filename  right_leg\n",
      "0  /kaggle/working/frames/44944_Stefani_449_swing...          1\n",
      "1  /kaggle/working/frames/44944_Stefani_449_swing...          1\n",
      "2  /kaggle/working/frames/44944_Stefani_449_swing...          1\n",
      "3  /kaggle/working/frames/44944_Stefani_449_swing...          1\n",
      "4  /kaggle/working/frames/44944_Stefani_449_swing...          1\n",
      "                                            filename  left_leg\n",
      "0  /kaggle/working/frames/44944_Stefani_449_swing...         0\n",
      "1  /kaggle/working/frames/44944_Stefani_449_swing...         0\n",
      "2  /kaggle/working/frames/44944_Stefani_449_swing...         0\n",
      "3  /kaggle/working/frames/44944_Stefani_449_swing...         0\n",
      "4  /kaggle/working/frames/44944_Stefani_449_swing...         0\n"
     ]
    }
   ],
   "source": [
    "train_image_paths = []\n",
    "train_foot_contacts = []\n",
    "test_image_paths = []\n",
    "test_foot_contacts = []\n",
    "\n",
    "# Global frame counter\n",
    "global_frame_count = 0\n",
    "max_frames = 6000\n",
    "\n",
    "# Function to process video files and extract frames and labels\n",
    "def process_videos(base_path, frames_base_dir, image_paths, foot_contacts):\n",
    "    global global_frame_count\n",
    "\n",
    "    for person_folder in os.listdir(base_path):\n",
    "        person_path = os.path.join(base_path, person_folder)\n",
    "\n",
    "        if os.path.isdir(person_path):\n",
    "            # Loop through each activity folder\n",
    "            for activity_folder in os.listdir(person_path):\n",
    "                activity_path = os.path.join(person_path, activity_folder)\n",
    "\n",
    "                if os.path.isdir(activity_path):\n",
    "                    # Path to the .npy file containing foot contact data\n",
    "                    foot_contacts_path = os.path.join(activity_path, 'foot_contacts.npy')\n",
    "\n",
    "                    if os.path.exists(foot_contacts_path):\n",
    "                        # Load foot contact labels\n",
    "                        foot_contact_labels = np.load(foot_contacts_path)\n",
    "\n",
    "                        # Construct the pattern to match video files\n",
    "                        video_pattern = f'{activity_folder}_view0.mp4'\n",
    "\n",
    "                        # Loop through each video file in the activity folder\n",
    "                        for video_file in os.listdir(activity_path):\n",
    "                            if video_file == video_pattern:\n",
    "                                video_path = os.path.join(activity_path, video_file)\n",
    "\n",
    "                                # Directory to save the frames of the current video\n",
    "                                frames_dir = os.path.join(frames_base_dir, f'{person_folder}_{activity_folder}')\n",
    "                                create_directory(frames_dir)\n",
    "\n",
    "                                # Capture the video from the file\n",
    "                                cap = cv2.VideoCapture(video_path)\n",
    "                                if not cap.isOpened():\n",
    "                                    print(f\"Error: Could not open video {video_path}\")\n",
    "                                    continue\n",
    "\n",
    "                                frame_count = 0\n",
    "                                while cap.isOpened() and global_frame_count < max_frames:\n",
    "                                    ret, frame = cap.read()\n",
    "                                    if not ret:\n",
    "                                        break\n",
    "\n",
    "                                    # Save frame as image\n",
    "                                    frame_filename = f'frame_{frame_count:04d}.jpg'\n",
    "                                    frame_path = os.path.join(frames_dir, frame_filename)\n",
    "                                    cv2.imwrite(frame_path, frame)\n",
    "\n",
    "                                    # Append frame path and corresponding foot contact label to lists\n",
    "                                    if frame_count < len(foot_contact_labels):\n",
    "                                        image_paths.append(frame_path)\n",
    "                                        foot_contacts.append(foot_contact_labels[frame_count])\n",
    "\n",
    "                                    frame_count += 1\n",
    "                                    global_frame_count += 1\n",
    "\n",
    "                                    # Stop processing if frame limit is reached\n",
    "                                    if global_frame_count >= max_frames:\n",
    "                                        print(\"Reached the frame limit of 6000.\")\n",
    "                                        break\n",
    "\n",
    "                                # Release the capture\n",
    "                                cap.release()\n",
    "\n",
    "                                print(f\"Processed {frame_count} frames from {video_file}\")\n",
    "\n",
    "                                # Stop further processing if frame limit is reached\n",
    "                                if global_frame_count >= max_frames:\n",
    "                                    return\n",
    "\n",
    "# Process the training data\n",
    "process_videos(base_path, frames_base_dir, train_image_paths, train_foot_contacts)\n",
    "\n",
    "# Create DataFrame for training data\n",
    "train_data = {\n",
    "    'filename': train_image_paths,\n",
    "    'left_heel': [label[0] for label in train_foot_contacts],\n",
    "    'left_toe': [label[1] for label in train_foot_contacts],\n",
    "    'right_heel': [label[2] for label in train_foot_contacts],\n",
    "    'right_toe': [label[3] for label in train_foot_contacts]\n",
    "}\n",
    "\n",
    "train_left_leg_values = [label[0] & label[1] for label in train_foot_contacts]\n",
    "train_right_leg_values = [label[2] & label[3] for label in train_foot_contacts]\n",
    "\n",
    "train_data_left_leg = {\n",
    "    'filename': train_image_paths,\n",
    "    'left_leg': train_left_leg_values\n",
    "}\n",
    "\n",
    "train_data_right_leg = {\n",
    "    'filename': train_image_paths,\n",
    "    'right_leg': train_right_leg_values\n",
    "}\n",
    "\n",
    "train_left_df = pd.DataFrame(train_data_left_leg)\n",
    "train_right_df = pd.DataFrame(train_data_right_leg)\n",
    "train_df = pd.DataFrame(train_data)\n",
    "\n",
    "print(train_df.head())\n",
    "print(train_right_df.head())\n",
    "print(train_left_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "447b0cda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T16:50:26.722609Z",
     "iopub.status.busy": "2024-09-30T16:50:26.722049Z",
     "iopub.status.idle": "2024-09-30T16:50:32.287277Z",
     "shell.execute_reply": "2024-09-30T16:50:32.286077Z"
    },
    "papermill": {
     "duration": 5.5791,
     "end_time": "2024-09-30T16:50:32.289331",
     "exception": false,
     "start_time": "2024-09-30T16:50:26.710231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 621 frames from jumping_video_prashant_gupta.mp4\n",
      "                                            filename  left_heel  left_toe  \\\n",
      "0  /kaggle/working/test-frames/jumping_video_pras...          0         1   \n",
      "1  /kaggle/working/test-frames/jumping_video_pras...          0         1   \n",
      "2  /kaggle/working/test-frames/jumping_video_pras...          0         1   \n",
      "3  /kaggle/working/test-frames/jumping_video_pras...          1         1   \n",
      "4  /kaggle/working/test-frames/jumping_video_pras...          1         1   \n",
      "\n",
      "   right_heel  right_toe  \n",
      "0           1          1  \n",
      "1           1          1  \n",
      "2           1          1  \n",
      "3           1          1  \n",
      "4           1          1  \n",
      "                                            filename  right_leg\n",
      "0  /kaggle/working/test-frames/jumping_video_pras...          1\n",
      "1  /kaggle/working/test-frames/jumping_video_pras...          1\n",
      "2  /kaggle/working/test-frames/jumping_video_pras...          1\n",
      "3  /kaggle/working/test-frames/jumping_video_pras...          1\n",
      "4  /kaggle/working/test-frames/jumping_video_pras...          1\n",
      "                                            filename  left_leg\n",
      "0  /kaggle/working/test-frames/jumping_video_pras...         0\n",
      "1  /kaggle/working/test-frames/jumping_video_pras...         0\n",
      "2  /kaggle/working/test-frames/jumping_video_pras...         0\n",
      "3  /kaggle/working/test-frames/jumping_video_pras...         1\n",
      "4  /kaggle/working/test-frames/jumping_video_pras...         1\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to hold the dataset\n",
    "test_image_paths = []\n",
    "test_foot_contacts = []\n",
    "\n",
    "def process_test_videos(test_dir, frame_test_dir, image_paths, foot_contacts):\n",
    "    for activity_folder in os.listdir(test_dir):\n",
    "        activity_path = os.path.join(test_dir, activity_folder)\n",
    "        \n",
    "        if os.path.isdir(activity_path):\n",
    "            # Path to the .npy file containing foot contact data\n",
    "            foot_contacts_path = os.path.join(activity_path, 'foot_contacts.npy')\n",
    "\n",
    "            if os.path.exists(foot_contacts_path):\n",
    "                # Load foot contact labels\n",
    "                foot_contact_labels = np.load(foot_contacts_path)\n",
    "\n",
    "                # Find the video file\n",
    "                video_file = f\"{activity_folder}.mp4\"\n",
    "                video_path = os.path.join(activity_path, video_file)\n",
    "\n",
    "                if os.path.exists(video_path):\n",
    "                    # Directory to save the frames of the current video\n",
    "                    frames_dir = os.path.join(frame_test_dir, activity_folder)\n",
    "                    create_directory(frames_dir)\n",
    "\n",
    "                    # Capture the video from the file\n",
    "                    cap = cv2.VideoCapture(video_path)\n",
    "                    if not cap.isOpened():\n",
    "                        print(f\"Error: Could not open video {video_path}\")\n",
    "                        continue\n",
    "\n",
    "                    frame_count = 0\n",
    "                    while cap.isOpened():\n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            break\n",
    "\n",
    "                        # Save frame as image\n",
    "                        frame_filename = f'frame_{frame_count:04d}.jpg'\n",
    "                        frame_path = os.path.join(frames_dir, frame_filename)\n",
    "                        cv2.imwrite(frame_path, frame)\n",
    "\n",
    "                        # Append frame path and corresponding foot contact label to lists\n",
    "                        if frame_count < len(foot_contact_labels):\n",
    "                            image_paths.append(frame_path)\n",
    "                            foot_contacts.append(foot_contact_labels[frame_count])\n",
    "                        frame_count += 1\n",
    "\n",
    "                    # Release the capture\n",
    "                    cap.release()\n",
    "\n",
    "                    print(f\"Processed {frame_count} frames from {video_file}\")\n",
    "\n",
    "process_test_videos(test_dir, frames_test_dir, test_image_paths, test_foot_contacts)\n",
    "# Prepare DataFrames for left and right leg\n",
    "test_data = {\n",
    "    'filename': test_image_paths,\n",
    "    'left_heel': [label[0] for label in test_foot_contacts],\n",
    "    'left_toe': [label[1] for label in test_foot_contacts],\n",
    "    'right_heel': [label[2] for label in test_foot_contacts],\n",
    "    'right_toe': [label[3] for label in test_foot_contacts]\n",
    "}\n",
    "\n",
    "test_left_leg_values = [label[0] & label[1] for label in test_foot_contacts]\n",
    "test_right_leg_values = [label[2] & label[3] for label in test_foot_contacts]\n",
    "\n",
    "test_data_left_leg = {\n",
    "    'filename': test_image_paths,\n",
    "    'left_leg': test_left_leg_values\n",
    "}\n",
    "\n",
    "test_data_right_leg = {\n",
    "    'filename': test_image_paths,\n",
    "    'right_leg': test_right_leg_values\n",
    "}\n",
    "\n",
    "test_left_df = pd.DataFrame(test_data_left_leg)\n",
    "test_right_df = pd.DataFrame(test_data_right_leg)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(test_df.head())\n",
    "print(test_right_df.head())\n",
    "print(test_left_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4174f2e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T16:50:32.313115Z",
     "iopub.status.busy": "2024-09-30T16:50:32.312643Z",
     "iopub.status.idle": "2024-09-30T16:50:32.984830Z",
     "shell.execute_reply": "2024-09-30T16:50:32.984055Z"
    },
    "papermill": {
     "duration": 0.686287,
     "end_time": "2024-09-30T16:50:32.986983",
     "exception": false,
     "start_time": "2024-09-30T16:50:32.300696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to load and preprocess image\n",
    "def load_and_preprocess_image(image_path, target_size=(224, 224)):\n",
    "    try:\n",
    "        img = cv2.imread(image_path.decode('utf-8'))\n",
    "        if img is None:\n",
    "            print(f\"Failed to load image at path: {image_path}\")\n",
    "            return np.zeros((target_size[0], target_size[1], 3), dtype=np.float32)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        h, w, _ = img.shape\n",
    "        if h > w:\n",
    "            pad_width = (h - w) // 2\n",
    "            padding = ((0, 0), (pad_width, h - w - pad_width), (0, 0))\n",
    "        else:\n",
    "            pad_height = (w - h) // 2\n",
    "            padding = ((pad_height, w - h - pad_height), (0, 0), (0, 0))\n",
    "        \n",
    "        img = np.pad(img, padding, mode='constant', constant_values=255)\n",
    "        img = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
    "        img = img / 255.0\n",
    "        return img.astype(np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image at path: {image_path}, error: {e}\")\n",
    "        return np.zeros((target_size[0], target_size[1], 3), dtype=np.float32)\n",
    "\n",
    "# Data Augmentation\n",
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\", input_shape=(224, 224, 3)),\n",
    "    ]\n",
    ")\n",
    "def create_dataset(image_paths, labels, batch_size, training=True):\n",
    "    def load_and_preprocess_image_tf(image_path, label):\n",
    "        img = tf.numpy_function(load_and_preprocess_image, [image_path], tf.float32)\n",
    "        img.set_shape((224, 224, 3))\n",
    "        return img, label\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.map(load_and_preprocess_image_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(buffer_size=len(image_paths))\n",
    "        dataset = dataset.map(lambda x, y: (data_augmentation(tf.expand_dims(x, 0))[0], y), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "071dde40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T16:50:33.010872Z",
     "iopub.status.busy": "2024-09-30T16:50:33.010316Z",
     "iopub.status.idle": "2024-09-30T16:50:33.280989Z",
     "shell.execute_reply": "2024-09-30T16:50:33.280226Z"
    },
    "papermill": {
     "duration": 0.284665,
     "end_time": "2024-09-30T16:50:33.283061",
     "exception": false,
     "start_time": "2024-09-30T16:50:32.998396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150528</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │   <span style=\"color: #00af00; text-decoration-color: #00af00\">154,141,696</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150528\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │   \u001b[38;5;34m154,141,696\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">154,805,249</span> (590.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m154,805,249\u001b[0m (590.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">154,801,665</span> (590.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m154,801,665\u001b[0m (590.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,584</span> (14.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,584\u001b[0m (14.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, optimizers\n",
    "# Define the MLP architecture\n",
    "model = models.Sequential([\n",
    "    layers.Flatten(input_shape=(224, 224, 3)), \n",
    "    layers.Dense(1024, activation='relu'),     \n",
    "    layers.BatchNormalization(),             \n",
    "    layers.Dropout(0.5),                      \n",
    "    layers.Dense(512, activation='relu'),       \n",
    "    layers.BatchNormalization(),               \n",
    "    layers.Dropout(0.5),                        \n",
    "    layers.Dense(256, activation='relu'),       \n",
    "    layers.BatchNormalization(),               \n",
    "    layers.Dropout(0.5),                        \n",
    "    layers.Dense(1, activation='sigmoid')      \n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model\n",
    "model.summary()#\n",
    "# Split data into training and validation sets\n",
    "#train_paths, val_paths, train_labels, val_labels = train_test_split(train_right_df['filename'].values, train_right_df['right_leg'].values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare datasets\n",
    "#train_dataset = create_dataset(train_paths, train_labels, batch_size=4, training=True)  # Smaller batch size\n",
    "#val_dataset = create_dataset(val_paths, val_labels, batch_size=4, training=False)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = create_dataset(train_right_df['filename'].values, train_right_df['right_leg'].values, batch_size=4, training=True)\n",
    "val_dataset = create_dataset(test_right_df['filename'].values, test_right_df['right_leg'].values, batch_size=4, training=False)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "\n",
    "\n",
    "#history = model.fit(train_dataset, validation_data=val_dataset, epochs=10, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04740c4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T16:50:33.308800Z",
     "iopub.status.busy": "2024-09-30T16:50:33.308077Z",
     "iopub.status.idle": "2024-09-30T17:18:26.597219Z",
     "shell.execute_reply": "2024-09-30T17:18:26.596353Z"
    },
    "papermill": {
     "duration": 1673.304281,
     "end_time": "2024-09-30T17:18:26.599552",
     "exception": false,
     "start_time": "2024-09-30T16:50:33.295271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m   7/1500\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 22ms/step - accuracy: 0.5628 - loss: 1.0695"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727715079.963349     471 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "W0000 00:00:1727715079.983569     471 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5582 - loss: 0.8836"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1727715111.995090     470 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 27ms/step - accuracy: 0.5582 - loss: 0.8836 - val_accuracy: 0.2287 - val_loss: 1.0957\n",
      "Epoch 2/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 26ms/step - accuracy: 0.6182 - loss: 0.6784 - val_accuracy: 0.7713 - val_loss: 0.5752\n",
      "Epoch 3/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 28ms/step - accuracy: 0.6318 - loss: 0.6594 - val_accuracy: 0.7713 - val_loss: 0.5258\n",
      "Epoch 4/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 26ms/step - accuracy: 0.6549 - loss: 0.6378 - val_accuracy: 0.7713 - val_loss: 0.6192\n",
      "Epoch 5/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 26ms/step - accuracy: 0.6475 - loss: 0.6324 - val_accuracy: 0.7713 - val_loss: 0.8513\n",
      "Epoch 6/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 26ms/step - accuracy: 0.6615 - loss: 0.6235 - val_accuracy: 0.7713 - val_loss: 0.7290\n",
      "Epoch 7/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 28ms/step - accuracy: 0.6390 - loss: 0.6343 - val_accuracy: 0.7713 - val_loss: 0.7045\n",
      "Epoch 8/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 28ms/step - accuracy: 0.6674 - loss: 0.6239 - val_accuracy: 0.7713 - val_loss: 1.1169\n",
      "Epoch 9/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 25ms/step - accuracy: 0.6598 - loss: 0.6273 - val_accuracy: 0.7713 - val_loss: 1.3550\n",
      "Epoch 10/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 26ms/step - accuracy: 0.6679 - loss: 0.6138 - val_accuracy: 0.7713 - val_loss: 1.7891\n",
      "Epoch 11/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 26ms/step - accuracy: 0.6628 - loss: 0.6168 - val_accuracy: 0.7713 - val_loss: 2.5885\n",
      "Epoch 12/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 26ms/step - accuracy: 0.6801 - loss: 0.5962 - val_accuracy: 0.2287 - val_loss: 173.6649\n",
      "Epoch 13/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 26ms/step - accuracy: 0.6738 - loss: 0.6064 - val_accuracy: 0.7713 - val_loss: 4.6444\n",
      "Epoch 14/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 26ms/step - accuracy: 0.6760 - loss: 0.6040 - val_accuracy: 0.2544 - val_loss: 109.2916\n",
      "Epoch 15/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 26ms/step - accuracy: 0.6761 - loss: 0.5957 - val_accuracy: 0.6876 - val_loss: 4.4766\n",
      "Epoch 16/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 26ms/step - accuracy: 0.6827 - loss: 0.5916 - val_accuracy: 0.2287 - val_loss: 331.3409\n",
      "Epoch 17/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 26ms/step - accuracy: 0.6901 - loss: 0.5817 - val_accuracy: 0.3124 - val_loss: 58.1755\n",
      "Epoch 18/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 28ms/step - accuracy: 0.6989 - loss: 0.5869 - val_accuracy: 0.7713 - val_loss: 31.7305\n",
      "Epoch 19/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 28ms/step - accuracy: 0.6938 - loss: 0.5769 - val_accuracy: 0.7713 - val_loss: 49.1923\n",
      "Epoch 20/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 26ms/step - accuracy: 0.7101 - loss: 0.5586 - val_accuracy: 0.3430 - val_loss: 34.2247\n",
      "Epoch 21/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 26ms/step - accuracy: 0.7124 - loss: 0.5636 - val_accuracy: 0.7713 - val_loss: 24.8820\n",
      "Epoch 22/300\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 26ms/step - accuracy: 0.6924 - loss: 0.5805 - val_accuracy: 0.7713 - val_loss: 63.6635\n",
      "Epoch 22: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n"
     ]
    }
   ],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',  # or 'val_loss'\n",
    "    patience=20,  # number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,\n",
    "    restore_best_weights=True  # restores the model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=300, verbose=1,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a895ad9",
   "metadata": {
    "papermill": {
     "duration": 0.89929,
     "end_time": "2024-09-30T17:18:28.453135",
     "exception": false,
     "start_time": "2024-09-30T17:18:27.553845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5502836,
     "sourceId": 9116609,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5510530,
     "sourceId": 9127310,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5561505,
     "sourceId": 9198955,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1733.253058,
   "end_time": "2024-09-30T17:18:32.244650",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-30T16:49:38.991592",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
