{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7560311",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-26T19:22:14.959037Z",
     "iopub.status.busy": "2024-08-26T19:22:14.958709Z",
     "iopub.status.idle": "2024-08-26T19:22:29.244609Z",
     "shell.execute_reply": "2024-08-26T19:22:29.243608Z"
    },
    "papermill": {
     "duration": 14.29281,
     "end_time": "2024-08-26T19:22:29.247037",
     "exception": false,
     "start_time": "2024-08-26T19:22:14.954227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score, f1_score, matthews_corrcoef, confusion_matrix\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc2f9b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T19:22:29.254206Z",
     "iopub.status.busy": "2024-08-26T19:22:29.253648Z",
     "iopub.status.idle": "2024-08-26T19:22:29.260410Z",
     "shell.execute_reply": "2024-08-26T19:22:29.259693Z"
    },
    "papermill": {
     "duration": 0.012294,
     "end_time": "2024-08-26T19:22:29.262343",
     "exception": false,
     "start_time": "2024-08-26T19:22:29.250049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "# Function to create directory if it does not exist\n",
    "def create_directory(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# Base path to the synthetic dataset\n",
    "base_path = '/kaggle/input/movement-dataset-entire/synthetic_dataset'\n",
    "# Path to save the frames\n",
    "frames_base_dir = '/kaggle/working/frames'\n",
    "\n",
    "test_dir ='/kaggle/input/entire-test-movement-dataset/Abhishek_prashant_input_data'\n",
    "frame_test_dir ='/kaggle/working/test/frames'\n",
    "\n",
    "# Create the directory to save frames\n",
    "create_directory(frames_base_dir)\n",
    "create_directory(frame_test_dir)\n",
    "# Initialize lists to hold the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "229e0a45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T19:22:29.269297Z",
     "iopub.status.busy": "2024-08-26T19:22:29.269016Z",
     "iopub.status.idle": "2024-08-26T19:23:00.551969Z",
     "shell.execute_reply": "2024-08-26T19:23:00.551022Z"
    },
    "papermill": {
     "duration": 31.289196,
     "end_time": "2024-08-26T19:23:00.554271",
     "exception": false,
     "start_time": "2024-08-26T19:22:29.265075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 60 frames from 449_swing_dancing_view0.mp4\n",
      "Processed 60 frames from 1847_baseball_walk_in_view0.mp4\n",
      "Processed 60 frames from 378_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 321_boxing_view0.mp4\n",
      "Processed 60 frames from 1854_baseball_step_up_to_bat_view0.mp4\n",
      "Processed 60 frames from 43_walking_view0.mp4\n",
      "Processed 60 frames from 189_baseball_pitching_view0.mp4\n",
      "Processed 60 frames from 391_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 2166_standing_turn_90_right_view0.mp4\n",
      "Processed 60 frames from 376_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 1855_baseball_milling_idle_view0.mp4\n",
      "Processed 60 frames from 2165_standing_turn_90_left_view0.mp4\n",
      "Processed 60 frames from 448_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 392_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 326_samba_dancing_view0.mp4\n",
      "Processed 60 frames from 1851_baseball_hit_view0.mp4\n",
      "Processed 60 frames from 1529_quarterback_pass_view0.mp4\n",
      "Processed 60 frames from 328_boxing_view0.mp4\n",
      "Processed 60 frames from 453_swing_dancing_view0.mp4\n",
      "Processed 60 frames from 1857_baseball_bunt_view0.mp4\n",
      "Processed 60 frames from 1856_baseball_strike_view0.mp4\n",
      "Processed 60 frames from 340_boxing_view0.mp4\n",
      "Processed 60 frames from 28_walking_view0.mp4\n",
      "Processed 60 frames from 27_walking_view0.mp4\n",
      "Processed 60 frames from 42_crouched_walking_view0.mp4\n",
      "Processed 60 frames from 1846_baseball_step_out_view0.mp4\n",
      "Processed 60 frames from 451_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 2202_standing_turn_left_90_view0.mp4\n",
      "Processed 60 frames from 44_walking_view0.mp4\n",
      "Processed 60 frames from 1844_baseball_idle_view0.mp4\n",
      "Processed 60 frames from 25_walking_view0.mp4\n",
      "Processed 60 frames from 373_samba_dancing_view0.mp4\n",
      "Processed 60 frames from 116_baseball_catcher_view0.mp4\n",
      "Processed 60 frames from 26_walking_view0.mp4\n",
      "Processed 60 frames from 396_samba_dancing_view0.mp4\n",
      "Processed 60 frames from 39_walking_view0.mp4\n",
      "Processed 60 frames from 293_swing_dancing_view0.mp4\n",
      "Processed 60 frames from 286_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 1245_idle_view0.mp4\n",
      "Processed 60 frames from 1848_baseball_walk_out_view0.mp4\n",
      "Processed 60 frames from 1845_baseball_step_in_view0.mp4\n",
      "Processed 60 frames from 1850_baseball_hit_view0.mp4\n",
      "Processed 60 frames from 327_samba_dancing_view0.mp4\n",
      "Processed 60 frames from 1246_idle_view0.mp4\n",
      "Processed 60 frames from 444_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 119_baseball_swing_view0.mp4\n",
      "Processed 60 frames from 131_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 2203_standing_turn_right_90_view0.mp4\n",
      "Processed 60 frames from 118_baseball_idle_view0.mp4\n",
      "Processed 60 frames from 393_samba_dancing_view0.mp4\n",
      "Processed 60 frames from 277_samba_dancing_view0.mp4\n",
      "Processed 60 frames from 1250_idle_view0.mp4\n",
      "Processed 60 frames from 1473_baseball_pitching_view0.mp4\n",
      "Processed 60 frames from 301_samba_dancing_view0.mp4\n",
      "Processed 60 frames from 445_swing_dancing_view0.mp4\n",
      "Processed 60 frames from 1849_baseball_hit_view0.mp4\n",
      "Processed 60 frames from 117_baseball_bunt_view0.mp4\n",
      "Processed 60 frames from 1192_idle_view0.mp4\n",
      "Processed 60 frames from 115_baseball_catcher_view0.mp4\n",
      "Processed 60 frames from 443_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 303_swing_dancing_view0.mp4\n",
      "Processed 60 frames from 285_samba_dancing_view0.mp4\n",
      "Processed 60 frames from 121_baseball_umpire_view0.mp4\n",
      "Processed 60 frames from 120_baseball_swing_inside_view0.mp4\n",
      "Processed 60 frames from 24_walking_view0.mp4\n",
      "Processed 60 frames from 449_swing_dancing_view0.mp4\n",
      "Processed 60 frames from 1847_baseball_walk_in_view0.mp4\n",
      "Processed 60 frames from 378_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 321_boxing_view0.mp4\n",
      "Processed 60 frames from 1854_baseball_step_up_to_bat_view0.mp4\n",
      "Processed 60 frames from 43_walking_view0.mp4\n",
      "Processed 60 frames from 189_baseball_pitching_view0.mp4\n",
      "Processed 60 frames from 391_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 2166_standing_turn_90_right_view0.mp4\n",
      "Processed 60 frames from 376_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 1855_baseball_milling_idle_view0.mp4\n",
      "Processed 60 frames from 2165_standing_turn_90_left_view0.mp4\n",
      "Processed 60 frames from 448_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 392_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 326_samba_dancing_view0.mp4\n",
      "Processed 60 frames from 1851_baseball_hit_view0.mp4\n",
      "Processed 60 frames from 1529_quarterback_pass_view0.mp4\n",
      "Processed 60 frames from 328_boxing_view0.mp4\n",
      "Processed 60 frames from 453_swing_dancing_view0.mp4\n",
      "Processed 60 frames from 1857_baseball_bunt_view0.mp4\n",
      "Processed 60 frames from 1856_baseball_strike_view0.mp4\n",
      "Processed 60 frames from 340_boxing_view0.mp4\n",
      "Processed 60 frames from 28_walking_view0.mp4\n",
      "Processed 60 frames from 27_walking_view0.mp4\n",
      "Processed 60 frames from 42_crouched_walking_view0.mp4\n",
      "Processed 60 frames from 1846_baseball_step_out_view0.mp4\n",
      "Processed 60 frames from 451_salsa_dancing_view0.mp4\n",
      "Processed 60 frames from 2202_standing_turn_left_90_view0.mp4\n",
      "Processed 60 frames from 44_walking_view0.mp4\n",
      "Processed 60 frames from 1844_baseball_idle_view0.mp4\n",
      "Processed 60 frames from 25_walking_view0.mp4\n",
      "Processed 60 frames from 373_samba_dancing_view0.mp4\n",
      "Processed 60 frames from 116_baseball_catcher_view0.mp4\n",
      "Processed 60 frames from 26_walking_view0.mp4\n",
      "Frame limit reached.\n",
      "Processed 60 frames from 396_samba_dancing_view0.mp4\n",
      "                                            filename  left_heel  left_toe  \\\n",
      "0  /kaggle/working/frames/44944_Stefani_449_swing...          0         0   \n",
      "1  /kaggle/working/frames/44944_Stefani_449_swing...          0         0   \n",
      "2  /kaggle/working/frames/44944_Stefani_449_swing...          0         0   \n",
      "3  /kaggle/working/frames/44944_Stefani_449_swing...          0         0   \n",
      "4  /kaggle/working/frames/44944_Stefani_449_swing...          0         0   \n",
      "\n",
      "   right_heel  right_toe  \n",
      "0           1          1  \n",
      "1           1          1  \n",
      "2           1          1  \n",
      "3           1          1  \n",
      "4           1          1  \n",
      "                                            filename  right_leg\n",
      "0  /kaggle/working/frames/44944_Stefani_449_swing...          1\n",
      "1  /kaggle/working/frames/44944_Stefani_449_swing...          1\n",
      "2  /kaggle/working/frames/44944_Stefani_449_swing...          1\n",
      "3  /kaggle/working/frames/44944_Stefani_449_swing...          1\n",
      "4  /kaggle/working/frames/44944_Stefani_449_swing...          1\n",
      "                                            filename  left_leg\n",
      "0  /kaggle/working/frames/44944_Stefani_449_swing...         0\n",
      "1  /kaggle/working/frames/44944_Stefani_449_swing...         0\n",
      "2  /kaggle/working/frames/44944_Stefani_449_swing...         0\n",
      "3  /kaggle/working/frames/44944_Stefani_449_swing...         0\n",
      "4  /kaggle/working/frames/44944_Stefani_449_swing...         0\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store paths and labels\n",
    "train_image_paths = []\n",
    "train_foot_contacts = []\n",
    "\n",
    "# Set a limit on the number of frames to process\n",
    "frame_limit = 6000\n",
    "total_frames = 0\n",
    "stop_processing = False\n",
    "\n",
    "# Loop through each person folder\n",
    "for person_folder in os.listdir(base_path):\n",
    "    if stop_processing:  # Check if processing should stop\n",
    "        break\n",
    "\n",
    "    person_path = os.path.join(base_path, person_folder)\n",
    "    if os.path.isdir(person_path):\n",
    "        # Loop through each activity folder\n",
    "        for activity_folder in os.listdir(person_path):\n",
    "            if stop_processing:  # Check if processing should stop\n",
    "                break\n",
    "\n",
    "            activity_path = os.path.join(person_path, activity_folder)\n",
    "            if os.path.isdir(activity_path):\n",
    "                # Path to the .npy file containing foot contact data\n",
    "                foot_contacts_path = os.path.join(activity_path, 'foot_contacts.npy')\n",
    "                \n",
    "                if os.path.exists(foot_contacts_path):\n",
    "                    # Load foot contact labels\n",
    "                    foot_contact_labels = np.load(foot_contacts_path)\n",
    "                    \n",
    "                    # Construct the pattern to match video files\n",
    "                    video_pattern = f'{activity_folder}_view0.mp4'\n",
    "                    \n",
    "                    # Loop through each video file in the activity folder\n",
    "                    for video_file in os.listdir(activity_path):\n",
    "                        if stop_processing:  # Check if processing should stop\n",
    "                            break\n",
    "\n",
    "                        if video_file == video_pattern:\n",
    "                            video_path = os.path.join(activity_path, video_file)\n",
    "                            \n",
    "                            # Directory to save the frames of the current video\n",
    "                            frames_dir = os.path.join(frames_base_dir, f'{person_folder}_{activity_folder}')\n",
    "                            os.makedirs(frames_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "                            \n",
    "                            # Capture the video from the file\n",
    "                            cap = cv2.VideoCapture(video_path)\n",
    "                            if not cap.isOpened():\n",
    "                                print(f\"Error: Could not open video {video_path}\")\n",
    "                                continue\n",
    "                            \n",
    "                            frame_count = 0\n",
    "                            while cap.isOpened():\n",
    "                                ret, frame = cap.read()\n",
    "                                if not ret:\n",
    "                                    break\n",
    "                                \n",
    "                                # Save frame as image\n",
    "                                frame_filename = f'frame_{frame_count:04d}.jpg'\n",
    "                                frame_path = os.path.join(frames_dir, frame_filename)\n",
    "                                cv2.imwrite(frame_path, frame)\n",
    "                                \n",
    "                                # Append frame path and corresponding foot contact label to lists\n",
    "                                if frame_count < len(foot_contact_labels):\n",
    "                                    train_image_paths.append(frame_path)\n",
    "                                    train_foot_contacts.append(foot_contact_labels[frame_count])\n",
    "                                \n",
    "                                frame_count += 1\n",
    "                                total_frames += 1\n",
    "                                \n",
    "                                # Check if the frame limit is reached\n",
    "                                if total_frames >= frame_limit:\n",
    "                                    stop_processing = True  # Set flag to stop further processing\n",
    "                                    print(\"Frame limit reached.\")\n",
    "                                    break\n",
    "                            \n",
    "                            # Release the capture\n",
    "                            cap.release()\n",
    "                            print(f\"Processed {frame_count} frames from {video_file}\")\n",
    "\n",
    "# Create DataFrame\n",
    "data = {\n",
    "    'filename': train_image_paths,\n",
    "    'left_heel': [label[0] for label in train_foot_contacts],\n",
    "    'left_toe': [label[1] for label in train_foot_contacts],\n",
    "    'right_heel': [label[2] for label in train_foot_contacts],\n",
    "    'right_toe': [label[3] for label in train_foot_contacts]\n",
    "}\n",
    "\n",
    "left_leg_values = [label[0] & label[1] for label in train_foot_contacts]\n",
    "right_leg_values = [label[2] & label[3] for label in train_foot_contacts]\n",
    "\n",
    "data_left_leg = {\n",
    "    'filename': train_image_paths,\n",
    "    'left_leg': left_leg_values\n",
    "}\n",
    "\n",
    "data_right_leg = {\n",
    "    'filename': train_image_paths,\n",
    "    'right_leg': right_leg_values\n",
    "}\n",
    "\n",
    "train_left_df = pd.DataFrame(data_left_leg)\n",
    "train_right_df = pd.DataFrame(data_right_leg)\n",
    "train_df = pd.DataFrame(data)\n",
    "\n",
    "print(train_df.head())\n",
    "print(train_right_df.head())\n",
    "print(train_left_df.head())\n",
    "print(len(train_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468c5d73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T19:23:00.577072Z",
     "iopub.status.busy": "2024-08-26T19:23:00.576313Z",
     "iopub.status.idle": "2024-08-26T19:23:08.689771Z",
     "shell.execute_reply": "2024-08-26T19:23:08.688661Z"
    },
    "papermill": {
     "duration": 8.12723,
     "end_time": "2024-08-26T19:23:08.691965",
     "exception": false,
     "start_time": "2024-08-26T19:23:00.564735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200 frames from /kaggle/input/entire-test-movement-dataset/Abhishek_prashant_input_data/Abhishek/jumping_abhishek.mp4\n",
      "                                            filename  left_leg  right_leg\n",
      "0  /kaggle/working/test/frames/jumping_abhishek_f...         1          1\n",
      "1  /kaggle/working/test/frames/jumping_abhishek_f...         1          1\n",
      "2  /kaggle/working/test/frames/jumping_abhishek_f...         1          1\n",
      "3  /kaggle/working/test/frames/jumping_abhishek_f...         1          1\n",
      "4  /kaggle/working/test/frames/jumping_abhishek_f...         1          1\n",
      "                                            filename  left_leg\n",
      "0  /kaggle/working/test/frames/jumping_abhishek_f...         1\n",
      "1  /kaggle/working/test/frames/jumping_abhishek_f...         1\n",
      "2  /kaggle/working/test/frames/jumping_abhishek_f...         1\n",
      "3  /kaggle/working/test/frames/jumping_abhishek_f...         1\n",
      "4  /kaggle/working/test/frames/jumping_abhishek_f...         1\n",
      "                                            filename  right_leg\n",
      "0  /kaggle/working/test/frames/jumping_abhishek_f...          1\n",
      "1  /kaggle/working/test/frames/jumping_abhishek_f...          1\n",
      "2  /kaggle/working/test/frames/jumping_abhishek_f...          1\n",
      "3  /kaggle/working/test/frames/jumping_abhishek_f...          1\n",
      "4  /kaggle/working/test/frames/jumping_abhishek_f...          1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "test_image_paths = []\n",
    "test_foot_contacts = []\n",
    "# Function to create a directory if it doesn't exist\n",
    "def create_directory(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# Directories\n",
    "test_dir = '/kaggle/input/entire-test-movement-dataset'\n",
    "frame_test_dir = '/kaggle/working/test/frames'\n",
    "create_directory(frame_test_dir)\n",
    "\n",
    "# Paths to MP4 files and corresponding ground truth CSV files\n",
    "video_paths = [\n",
    "    '/kaggle/input/entire-test-movement-dataset/Abhishek_prashant_input_data/Abhishek/jumping_abhishek.mp4',\n",
    "    #'/kaggle/input/entire-test-movement-dataset/Abhishek_prashant_input_data/Prashant/jumping_video_prashant_gupta.mp4'\n",
    "]\n",
    "\n",
    "ground_truth_paths = [\n",
    "    '/kaggle/input/entire-test-movement-dataset/Abhishek_prashant_input_data/Abhishek/ground_truth/abhishek jumping video.csv',\n",
    "    #'/kaggle/input/entire-test-movement-dataset/Abhishek_prashant_input_data/Prashant/ground_truth/prashant jumping video.csv'\n",
    "]\n",
    "\n",
    "# Initialize lists for DataFrame\n",
    "test_image_paths = []\n",
    "test_foot_contacts = []\n",
    "\n",
    "# Set a limit on the number of frames to process\n",
    "MAX_FRAMES = 200\n",
    "total_frames_processed = 0\n",
    "\n",
    "# Process each video and its corresponding ground truth\n",
    "for video_path, gt_path in zip(video_paths, ground_truth_paths):\n",
    "    if total_frames_processed >= MAX_FRAMES:\n",
    "        break\n",
    "\n",
    "    # Read the ground truth CSV\n",
    "    ground_truth_df = pd.read_csv(gt_path)\n",
    "    \n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Extract video identifier (base name of video file without extension)\n",
    "    video_id = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    \n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        if total_frames_processed >= MAX_FRAMES:\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Save frame as image with video identifier\n",
    "        frame_filename = f'{video_id}_frame_{total_frames_processed:04d}.jpg'\n",
    "        frame_path = os.path.join(frame_test_dir, frame_filename)\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        \n",
    "        # Get foot contact data for the current frame\n",
    "        if frame_count < len(ground_truth_df):\n",
    "            left_foot = ground_truth_df.iloc[frame_count]['left_foot']\n",
    "            right_foot = ground_truth_df.iloc[frame_count]['right_foot']\n",
    "            test_image_paths.append(frame_path)\n",
    "            test_foot_contacts.append([left_foot, right_foot])\n",
    "        \n",
    "        frame_count += 1\n",
    "        total_frames_processed += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Processed {frame_count} frames from {video_path}\")\n",
    "\n",
    "    if total_frames_processed >= MAX_FRAMES:\n",
    "        break\n",
    "\n",
    "# Create DataFrames\n",
    "test_df = pd.DataFrame({\n",
    "    'filename': test_image_paths,\n",
    "    'left_leg': [label[0] for label in test_foot_contacts],\n",
    "    'right_leg': [label[1] for label in test_foot_contacts]\n",
    "})\n",
    "\n",
    "# Create DataFrames for left and right foot separately if needed\n",
    "test_left_df = pd.DataFrame({\n",
    "    'filename': test_image_paths,\n",
    "    'left_leg': [label[0] for label in test_foot_contacts]\n",
    "})\n",
    "\n",
    "test_right_df = pd.DataFrame({\n",
    "    'filename': test_image_paths,\n",
    "    'right_leg': [label[1] for label in test_foot_contacts]\n",
    "})\n",
    "\n",
    "# Print the first few rows of each DataFrame\n",
    "print(test_df.head())\n",
    "print(test_left_df.head())\n",
    "print(test_right_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f6e803c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T19:23:08.715363Z",
     "iopub.status.busy": "2024-08-26T19:23:08.714797Z",
     "iopub.status.idle": "2024-08-26T20:00:40.103051Z",
     "shell.execute_reply": "2024-08-26T20:00:40.102174Z"
    },
    "papermill": {
     "duration": 2251.402575,
     "end_time": "2024-08-26T20:00:40.105490",
     "exception": false,
     "start_time": "2024-08-26T19:23:08.702915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94668760/94668760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resnet50v2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,564,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resnet50v2 (\u001b[38;5;33mFunctional\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,564,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │         \u001b[38;5;34m8,192\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m524,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,097,793</span> (91.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,097,793\u001b[0m (91.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,999,873</span> (68.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,999,873\u001b[0m (68.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,097,920</span> (23.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m6,097,920\u001b[0m (23.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1724700234.334137     468 service.cc:145] XLA service 0x7dc960003c30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1724700234.334205     468 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   5/1200\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 30ms/step - accuracy: 0.5525 - loss: 0.6788"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1724700245.812772     468 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 37ms/step - accuracy: 0.6037 - loss: 0.6575 - val_accuracy: 0.6600 - val_loss: 0.5953\n",
      "Epoch 2/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 36ms/step - accuracy: 0.6504 - loss: 0.6164 - val_accuracy: 0.6483 - val_loss: 0.6050\n",
      "Epoch 3/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 36ms/step - accuracy: 0.6865 - loss: 0.5910 - val_accuracy: 0.7000 - val_loss: 0.5902\n",
      "Epoch 4/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 34ms/step - accuracy: 0.6976 - loss: 0.5751 - val_accuracy: 0.7133 - val_loss: 0.6215\n",
      "Epoch 5/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 34ms/step - accuracy: 0.7166 - loss: 0.5468 - val_accuracy: 0.7383 - val_loss: 0.5479\n",
      "Epoch 6/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 33ms/step - accuracy: 0.7350 - loss: 0.5205 - val_accuracy: 0.7308 - val_loss: 0.5856\n",
      "Epoch 7/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 34ms/step - accuracy: 0.7459 - loss: 0.4967 - val_accuracy: 0.7467 - val_loss: 0.6321\n",
      "Epoch 8/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 34ms/step - accuracy: 0.7816 - loss: 0.4551 - val_accuracy: 0.8092 - val_loss: 0.4146\n",
      "Epoch 9/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 36ms/step - accuracy: 0.7919 - loss: 0.4295 - val_accuracy: 0.8158 - val_loss: 0.4399\n",
      "Epoch 10/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 34ms/step - accuracy: 0.8067 - loss: 0.4297 - val_accuracy: 0.8150 - val_loss: 0.4214\n",
      "Epoch 11/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 36ms/step - accuracy: 0.8224 - loss: 0.3941 - val_accuracy: 0.8283 - val_loss: 0.3909\n",
      "Epoch 12/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 34ms/step - accuracy: 0.8486 - loss: 0.3531 - val_accuracy: 0.8375 - val_loss: 0.4058\n",
      "Epoch 13/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 34ms/step - accuracy: 0.8388 - loss: 0.3590 - val_accuracy: 0.8450 - val_loss: 0.3453\n",
      "Epoch 14/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 34ms/step - accuracy: 0.8484 - loss: 0.3426 - val_accuracy: 0.8458 - val_loss: 0.3903\n",
      "Epoch 15/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 34ms/step - accuracy: 0.8666 - loss: 0.3178 - val_accuracy: 0.8475 - val_loss: 0.3577\n",
      "Epoch 16/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 34ms/step - accuracy: 0.8583 - loss: 0.3150 - val_accuracy: 0.8458 - val_loss: 0.3817\n",
      "Epoch 17/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 36ms/step - accuracy: 0.8712 - loss: 0.3162 - val_accuracy: 0.8217 - val_loss: 0.4030\n",
      "Epoch 18/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 34ms/step - accuracy: 0.8715 - loss: 0.3107 - val_accuracy: 0.8525 - val_loss: 0.3612\n",
      "Epoch 19/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 34ms/step - accuracy: 0.8791 - loss: 0.2914 - val_accuracy: 0.8658 - val_loss: 0.3340\n",
      "Epoch 20/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 33ms/step - accuracy: 0.8856 - loss: 0.2770 - val_accuracy: 0.8675 - val_loss: 0.3136\n",
      "Epoch 21/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 35ms/step - accuracy: 0.8850 - loss: 0.2770 - val_accuracy: 0.8508 - val_loss: 0.3765\n",
      "Epoch 22/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 35ms/step - accuracy: 0.8941 - loss: 0.2486 - val_accuracy: 0.8775 - val_loss: 0.3360\n",
      "Epoch 23/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 34ms/step - accuracy: 0.9035 - loss: 0.2434 - val_accuracy: 0.8792 - val_loss: 0.3051\n",
      "Epoch 24/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 34ms/step - accuracy: 0.8921 - loss: 0.2559 - val_accuracy: 0.8675 - val_loss: 0.3421\n",
      "Epoch 25/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 34ms/step - accuracy: 0.8921 - loss: 0.2477 - val_accuracy: 0.8825 - val_loss: 0.3090\n",
      "Epoch 26/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 34ms/step - accuracy: 0.9064 - loss: 0.2401 - val_accuracy: 0.8783 - val_loss: 0.3185\n",
      "Epoch 27/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 34ms/step - accuracy: 0.9033 - loss: 0.2421 - val_accuracy: 0.8992 - val_loss: 0.2637\n",
      "Epoch 28/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 34ms/step - accuracy: 0.9013 - loss: 0.2379 - val_accuracy: 0.8992 - val_loss: 0.2594\n",
      "Epoch 29/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 34ms/step - accuracy: 0.9215 - loss: 0.1979 - val_accuracy: 0.8750 - val_loss: 0.3336\n",
      "Epoch 30/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 34ms/step - accuracy: 0.9206 - loss: 0.2164 - val_accuracy: 0.8775 - val_loss: 0.3268\n",
      "Epoch 31/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 34ms/step - accuracy: 0.9154 - loss: 0.2066 - val_accuracy: 0.8858 - val_loss: 0.2676\n",
      "Epoch 32/150\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 34ms/step - accuracy: 0.9265 - loss: 0.1935 - val_accuracy: 0.8967 - val_loss: 0.3041\n",
      "Epoch 32: early stopping\n",
      "Restoring model weights from the end of the best epoch: 27.\n"
     ]
    }
   ],
   "source": [
    "# Function to load and preprocess image\n",
    "def load_and_preprocess_image(image_path, target_size=(224, 224)):\n",
    "    try:\n",
    "        img = cv2.imread(image_path.decode('utf-8'))\n",
    "        if img is None:\n",
    "            print(f\"Failed to load image at path: {image_path}\")\n",
    "            return np.zeros((target_size[0], target_size[1], 3), dtype=np.float32)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        h, w, _ = img.shape\n",
    "        if h > w:\n",
    "            pad_width = (h - w) // 2\n",
    "            padding = ((0, 0), (pad_width, h - w - pad_width), (0, 0))\n",
    "        else:\n",
    "            pad_height = (w - h) // 2\n",
    "            padding = ((pad_height, w - h - pad_height), (0, 0), (0, 0))\n",
    "        \n",
    "        img = np.pad(img, padding, mode='constant', constant_values=255)\n",
    "        img = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
    "        img = img / 255.0\n",
    "        return img.astype(np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image at path: {image_path}, error: {e}\")\n",
    "        return np.zeros((target_size[0], target_size[1], 3), dtype=np.float32)\n",
    "\n",
    "# Data Augmentation\n",
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\", input_shape=(224, 224, 3)),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def create_dataset(image_paths, labels, batch_size, training=True):\n",
    "    def load_and_preprocess_image_tf(image_path, label):\n",
    "        img = tf.numpy_function(load_and_preprocess_image, [image_path], tf.float32)\n",
    "        img.set_shape((224, 224, 3))\n",
    "        return img, label\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.map(load_and_preprocess_image_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(buffer_size=len(image_paths))\n",
    "        dataset = dataset.map(lambda x, y: (data_augmentation(tf.expand_dims(x, 0))[0], y), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Load the base model\n",
    "\n",
    "base_model = tf.keras.applications.ResNet50V2(  #VGG16\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "# Set the last 10% of layers to be trainable\n",
    "total_layers = len(base_model.layers)\n",
    "trainable_layers_start = int(total_layers * 0.67)\n",
    "for layer in base_model.layers[:trainable_layers_start]:\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[trainable_layers_start:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    layers.BatchNormalization(),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and custom learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.build((None, 224, 224, 3)) \n",
    "# Print the summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(train_left_df['filename'].values, train_left_df['left_leg'].values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = create_dataset(train_paths, train_labels, batch_size=4, training=True)  # Smaller batch size\n",
    "val_dataset = create_dataset(val_paths, val_labels, batch_size=4, training=False)\n",
    "\n",
    "# Train the model\n",
    "steps_per_epoch = len(train_paths) // 4\n",
    "validation_steps = len(val_paths) // 4\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',  # or 'val_loss'\n",
    "    patience=5,  # number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,\n",
    "    restore_best_weights=True  # restores the model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=150, verbose=1,callbacks=[early_stopping])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5561505,
     "sourceId": 9198955,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5564247,
     "sourceId": 9203016,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2326.387678,
   "end_time": "2024-08-26T20:00:58.579701",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-26T19:22:12.192023",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
